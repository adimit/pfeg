\documentclass[draft,12pt]{article}

\usepackage{fourier}
\usepackage[english]{babel}

\usepackage[round]{natbib}
\usepackage[utf8]{inputenc}
\usepackage{linguex}

\usepackage[breaklinks,colorlinks,citecolor=blue]{hyperref}

\author{Aleksandar Dimitrov}
\title{Predicting Functional Elements in German}

\begin{document}
\bibliographystyle{plainnat}

\maketitle

\newcommand{\pair}[1]{\ensuremath{\langle #1 \rangle}}

\tableofcontents

\section{Abstract}

The prediction correct prepositions and other functional elements poses challenges
for interactive computer-aided language learning (iCALL) systems as well as
grammar checkers. While current work is mostly focused on English, the present
article extends a system first developed by \citep{elghafariETAL2010} to cover
German prepositions. An unsophisticated surface-based approach is established as
a baseline for more advanced future work.

\section{Introduction}

Functional elements (FEs), such as prepositions, typically carry little meaning, if
any. Their distributional properties should thus be inferable from context,
except in certain cases, later discussed in~\ref{sec:eqclass}.
To this end, many current approaches employ machine learning algorithms to infer
FE usage from linguistic features of the context. This technique naturally
focusses on selecting optimal contextual features to maximise the quality of
prediction.

SOME PAPERS % FIXME, cite these guys
report high accuracy figures for predicting prepositions and functional elements
in English. German has a richer morphology than English, which can make
contextual information more precise, but also may introduce a data sparsity
problem. Furthermore, German syntax allows for a rather free word order, as well
as frequent long distance dependency constructions, which may interfere with the
success of a simple surface-based method.

In line with~\citep{elghafariETAL2010} we show that comparable results can be
achieved for German using only shallow surface-based matching. Instead of a
machine learning algorithm, context data can be assembled in a large database
and then matched against in test cases.

In order to explore the value of linguistic features beyond the surface, we will
add information about the lemmas and part-of-speech tags of all contexts and
attempt to improve the surface-based results with this shallow linguistic data.

\subsection{Equivalence Classes} % FIXME: don't leave this in if we're not going
\label{sec:eqclass}              % care about equivalence classes.
Certain prepositions can build equivalence classes, where
predicting the exactly right one is hardly possible without knowing the authors
intent, which can be obscured by eliding the preposition in question.

\ex. Mary found her keys \textit{\{next to, inside, below, on, under,\ldots{}\}} the box.
\label{ex:keys}

In some cases, it might not even be inferable from context \textit{which exact}
preposition to use, since the right choice would not contribute to the
narrative. SOMEBODY % FIXME: Was it tetreault? Gamon?
cites an inter-annotator agreement study where two humans could only achieve a
RATHER LOW FIGURE. % FIXME: Yeah, this has to be filled in.
This leads us to believe that singleton preposition predictions might not always
be advisable, and that some contexts, such as the one in~\ref{ex:keys} call for
the prediction of \textit{equivalence classes} of prepositions.

\subsection{Prior Work}

\section{Method}

The databases are populated with context information gathered from the
part-of-speech tagged DEWAC corpus \citep{baroniETAL2008}. The corpus represents
a sample of the German Internet-landscape and has a total size of ABSURDLY HIGH
NUMBER. % FIXME: measure the size in words and sentences.
While the data in the DEWAC corpus can be noisy, avoiding data sparsity
problems seemed paramount, which justifies using the bigger DEWAC
corpus over smaller, less noisy corpora.

\subsection{Engineering the Software}
The size of the corpus and the quality of its data pose some engineering
challenges that the software had to overcome. The exploratory nature of the
research introduces a certain speed requirement to the training and testing
tasks, since they had to be redone rather often.

% FIXME: is this still true?
Currently, a complete training run on the entire DEWAC corpus on recent hardware
takes around two days.  The resulting data size is close to the original size of
the corpus (around 20 GiB.) What constricts the processing speed is mostly hard
drive access, since the databases can grow rather large.

The database used is SQLite\footnote{\url{http://www.sqlite.org}}, which was chosen for its
simplicity and relatively good performance. The software is written
in the highly parallelizable functional programming language
Haskell\footnote{\url{http://www.haskell.org}} and is available under a BSD3
license online\footnote{\url{http://github.com/adimit/PFEG}}.

\subsection{The Matching Algorithm}

Not always will a direct surface match be possible, since test cases will of
course contain unknown data. In these cases, rather than choosing the baseline,
a clever backoff algorithm which reduces window size or consults linguistic
features can improve accuracy.

\subsection{Data Format}

The context database is made up of two relations in Boyce-Codd Normal Form,
which are typically joined for most tasks. Both relations use integer values
exclusively, since database performance was observed to drop rapidly once
strings were inserted in large quantities.

\begin{figure}
ctxt id s1 s2 s3 s4 s5 s6 l1 l2 l3 l4 l5 l6 p1 p2 p3 p4 p5 p6

target id c i t
\caption{Database schema.}
\end{figure}

\subsubsection{Recording}

   The current data format facilitates incremental matching.
   Given a text \[[a,b,c,t,d,e,f]\] where $t$ is the target FE, and $a\ldots{}f$ are its
   surrounding tokens, the database format records the context in symmetric
   ordered pairs, making three to four entries per item, as shown
   in~\ref{fig:tables}.

\begin{figure*}
\caption{The 4 database tables generated in training.}
\label{fig:tables}
\begin{tabular}{rccccccccccc}
cT: &c3=P(\pair{a,f}) &c2=P(\pair{b,e}) &c1=P(\pair{c,d}) &T &count &id&\\
cL: &c3=L(\pair{a,f}) &c2=L(\pair{b,e}) &c1=L(\pair{c,d}) &T &count &id& ref-cT\\
cC: &c3=C(\pair{a,f}) &c2=C(\pair{b,e}) &c1=C(\pair{c,d}) &T &count &  & ref-cL\\
cS: &c3=S(\pair{a,f}) &c2=S(\pair{b,e}) &c1=S(\pair{c,d}) &T &count &  & ref-cL\\
\end{tabular}
\end{figure*}

   In order to reduce insertion and lookup times, we make 4 different tables
   for these different kinds of entries. P extracts the pos tags, L the lemmas,
   S the surface forms and C the surface forms with CARD and NNP tags
   substituted.

   Uniqueness constrains are imposed as follows: UNIQUE(c3,c2,c1,T,ref-*)

   c3,c2,c1 are stored in the database as a binary blob, using the following
   function to transform a pair of tokens into a unique per-pair identifier:

   \[ \mathit{mkC} \mbox{ a } \mbox{b } = \mathit{concatBytes24} [(\mathit{getID} a),(\mathit{getID} b)] \]

   getID retrieves the unigram id, and transforms it into a 24-bit integer.  The
   two 3-byte results are then concatenated into a 6-byte string. The inverse
   function unC splits 6 bytes into two 3-byte strings and looks up the
   corresponding integer indices in the unigram index. Technically speaking, the
   format of C in the three tables isn't (a,b), where a and b are strings, but
   instead just one 6-byte string. The majority of tokens in the DEWAC corpus
   occupy more than 3 bytes (the notable exceptions are punctuation marks and
   most functional elements.)

   This way, we can save some space in the context database, which is unusually
   big. T is stored as a plain text string.

\subsubsection{Matching}
   The matching algorithm progresses inward, opposite to the recording routine.
   Given a query item [a,b,c,?,d,e,f], aâ€¦f again denoting context.

\section{Evaluation}

\section{Conclusion}

\subsection{Future Work}

The current design relies on huge databases to perform. This is slow to train
and cumbersome to distribute. A smaller and faster lossy language model might at
least alleviate the redistribution concern without sacrificing in accuracy.

The current research was limited to just a handful of prepositions. Partly, this
was done to keep it comparable to current efforts in the literature, which will
typically restrict the list of predicted functional elements to one or two
dozen. It is also not too big a concern, since we used the most frequent
prepositions in the DEWAC corpus and Zipf's law %FIXME CITE THIS
states that these must account for an overwhelming amount of overall
prepositions. % FIXME Maybe show some numbers here, like, what are the prep tags
              % in the tag set and then go and make a pretty graph out of that.

Articles are also a big concern for German second-language acquisition. Even
very proficient speakers tend to make most of their mistakes with articles,
choosing either wrong gender or case, or using or omitting them where it is
inappropriate to do so. SOMEBODY % FIXME was it Tetreault?
showed article prediction for English, but in German the task is much more
sophisticated because the articles also carry case markings. It would be
interesting to explore the viability of a surface-based approach to predicting
articles.

\bibliography{/home/adimit/doc/lib/bib/main.bib}

\end{document}
